{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T22:15:42.556090Z",
     "start_time": "2025-07-21T22:13:51.735756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- Paths ---\n",
    "cwd = os.getcwd()\n",
    "geo_dir = os.path.join(cwd, \"geo\")\n",
    "os.makedirs(geo_dir, exist_ok=True)\n",
    "\n",
    "data_file = os.path.join(os.path.dirname(cwd), \"data\", \"fire_archive_M-C61_626683.csv.xz\")\n",
    "\n",
    "# --- URLs and files for 2024 TIGER/Line shapefiles ---\n",
    "base_url = \"https://www2.census.gov/geo/tiger/TIGER2024\"\n",
    "\n",
    "shapefiles = {\n",
    "    \"states\": f\"{base_url}/STATE/tl_2024_us_state.zip\",\n",
    "    \"counties\": f\"{base_url}/COUNTY/tl_2024_us_county.zip\",\n",
    "}\n",
    "\n",
    "# List of valid FIPS codes for states (1-56 excluding invalid ones)\n",
    "valid_fips = [f\"{i:02d}\" for i in range(1, 79) if i not in [3, 7, 14, 43, 52, 57, 58, 59, 61, 62, 63, 64, 65, 67, 68, 70, 71, 73, 74, 75, 76, 77]]\n",
    "\n",
    "# --- Download and extract function ---\n",
    "def download_and_extract(url, extract_to):\n",
    "    filename = os.path.basename(url)\n",
    "    dest_path = os.path.join(extract_to, filename)\n",
    "\n",
    "    # Download zip if not exists\n",
    "    if not os.path.exists(dest_path):\n",
    "        print(f\"Downloading {filename} ...\")\n",
    "        try:\n",
    "            r = requests.get(url, verify=False)\n",
    "            r.raise_for_status()\n",
    "            with open(dest_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"Downloaded {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed downloading {filename}: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"{filename} already downloaded.\")\n",
    "\n",
    "    # Extract shapefile components if .shp does not exist yet\n",
    "    shp_file = filename.replace(\".zip\", \".shp\")\n",
    "    shp_path = os.path.join(extract_to, shp_file)\n",
    "    if not os.path.exists(shp_path):\n",
    "        print(f\"Extracting {filename} ...\")\n",
    "        with zipfile.ZipFile(dest_path) as z:\n",
    "            for file in z.namelist():\n",
    "                if file.endswith((\".shp\", \".shx\", \".dbf\", \".prj\")):\n",
    "                    z.extract(file, extract_to)\n",
    "        print(f\"Extracted shapefiles from {filename}\")\n",
    "    else:\n",
    "        print(f\"{shp_file} already extracted.\")\n",
    "    return True\n",
    "\n",
    "# --- Download states and counties ---\n",
    "for name, url in shapefiles.items():\n",
    "    download_and_extract(url, geo_dir)\n",
    "\n",
    "# --- Download places shapefiles ---\n",
    "for fips in valid_fips:\n",
    "    url = f\"{base_url}/PLACE/tl_2024_{fips}_place.zip\"\n",
    "    download_and_extract(url, geo_dir)\n",
    "\n",
    "# --- Load fire data ---\n",
    "print(\"Loading fire data...\")\n",
    "data = pd.read_csv(data_file)\n",
    "data['acq_date']=pd.to_datetime(data['acq_date'])\n",
    "# Create datetime\n",
    "data['acq_datetime'] = (\n",
    "    data['acq_date'] +\n",
    "    pd.to_timedelta(data['acq_time'] // 100, unit='h') +\n",
    "    pd.to_timedelta(data['acq_time'] % 100, unit='m')\n",
    ")\n",
    "data['acq_datetime'] = data['acq_datetime'].dt.normalize()\n",
    "\n",
    "data.drop(['acq_time', 'instrument'], axis=1, inplace=True)\n",
    "data['confidence_binned'] = pd.cut(data['confidence'], bins=[-1, 30, 80, 101], labels=['l', 'n', 'h'])\n",
    "\n",
    "# Create GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(data['longitude'], data['latitude'])]\n",
    "fire_gdf = gpd.GeoDataFrame(data, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# --- Load shapefiles ---\n",
    "\n",
    "print(\"Loading states...\")\n",
    "states_path = os.path.join(geo_dir, \"tl_2024_us_state.shp\")\n",
    "states_gdf = gpd.read_file(states_path)[[\"NAME\", \"geometry\"]]\n",
    "states_gdf = states_gdf.rename(columns={\"NAME\": \"state_name\"}).to_crs(epsg=4326)\n",
    "\n",
    "print(\"Loading counties...\")\n",
    "counties_path = os.path.join(geo_dir, \"tl_2024_us_county.shp\")\n",
    "counties_gdf = gpd.read_file(counties_path)[[\"NAME\", \"geometry\"]]\n",
    "counties_gdf = counties_gdf.rename(columns={\"NAME\": \"county_name\"}).to_crs(epsg=4326)\n",
    "\n",
    "print(\"Loading places...\")\n",
    "place_gdfs = []\n",
    "for file in os.listdir(geo_dir):\n",
    "    if file.startswith(\"tl_2024_\") and file.endswith(\"_place.shp\"):\n",
    "        gdf = gpd.read_file(os.path.join(geo_dir, file))[[\"NAME\", \"geometry\"]]\n",
    "        gdf = gdf.rename(columns={\"NAME\": \"place_name\"})\n",
    "        place_gdfs.append(gdf)\n",
    "\n",
    "places_gdf = pd.concat(place_gdfs, ignore_index=True)\n",
    "places_gdf = gpd.GeoDataFrame(places_gdf, crs=\"EPSG:4269\").to_crs(epsg=4326)\n",
    "\n",
    "# --- Spatial joins ---\n",
    "\n",
    "print(\"Mapping fire points to states...\")\n",
    "fire_gdf = gpd.sjoin(fire_gdf, states_gdf, how=\"left\", predicate=\"within\")\n",
    "fire_gdf = fire_gdf.drop(columns=['index_right'])\n",
    "\n",
    "print(\"Mapping fire points to counties...\")\n",
    "fire_gdf = gpd.sjoin(fire_gdf, counties_gdf, how=\"left\", predicate=\"within\")\n",
    "fire_gdf = fire_gdf.drop(columns=['index_right'])\n",
    "\n",
    "print(\"Mapping fire points to places...\")\n",
    "fire_gdf = gpd.sjoin(fire_gdf, places_gdf, how=\"left\", predicate=\"within\")\n",
    "fire_gdf = fire_gdf.drop(columns=['index_right'])\n",
    "\n",
    "data=fire_gdf\n",
    "# --- Drop unnecessary columns ---\n",
    "for col in ['STATEFP', 'COUNTYFP', 'PLACEFP']:\n",
    "    if col in fire_gdf.columns:\n",
    "        fire_gdf = fire_gdf.drop(columns=col)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Earth radius in km\n",
    "kms_per_radian = 6371.0088\n",
    "radius_km = 0.75  # adjust for clustering range\n",
    "eps = radius_km / kms_per_radian\n",
    "\n",
    "results = []\n",
    "\n",
    "for date, group in data.groupby('acq_date'):\n",
    "    coords_rad = np.radians(group[['latitude', 'longitude']].values)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=3, algorithm='ball_tree', metric='haversine')\n",
    "    labels = db.fit_predict(coords_rad)\n",
    "\n",
    "    group = group.copy()\n",
    "    group['cluster'] = labels\n",
    "    group = group[group['cluster'] != -1]  # drop noise\n",
    "\n",
    "    # Fire ID is now unique per date + cluster\n",
    "    group['fire_id'] = group['acq_date'].astype(str) + '_C' + group['cluster'].astype(str)\n",
    "    results.append(group)\n",
    "\n",
    "# Combine all daily clustered results\n",
    "clustered_data = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Summary per fire_id\n",
    "summary = (\n",
    "    clustered_data\n",
    "    .groupby('fire_id')\n",
    "    .agg(\n",
    "        fire_count=('fire_id', 'count'),\n",
    "        mean_confidence=('confidence', 'mean'),\n",
    "        state_name=('state_name', 'first'),\n",
    "        county_name=('county_name', 'first'),\n",
    "        place_name=('place_name', 'first'),\n",
    "        acq_date=('acq_date', 'first')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary['cluster_confidence_binned']=pd.cut(summary['mean_confidence'],bins=[-1,30,80,101],labels=['Low','Nominal','High'])\n",
    "summary['fire_count_binned']=pd.cut(summary['fire_count'],bins=[-1,15,50,1000],labels=['Small','Medium','Large'])\n",
    "\n",
    "output=os.path.join(cwd, \"fire_data_enriched.csv\")\n",
    "summary.to_csv(output)\n",
    "print(f\"Saved data to {output}.\")"
   ],
   "id": "dd1955a91e6b5250",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tl_2024_us_state.zip already downloaded.\n",
      "tl_2024_us_state.shp already extracted.\n",
      "tl_2024_us_county.zip already downloaded.\n",
      "tl_2024_us_county.shp already extracted.\n",
      "tl_2024_01_place.zip already downloaded.\n",
      "tl_2024_01_place.shp already extracted.\n",
      "tl_2024_02_place.zip already downloaded.\n",
      "tl_2024_02_place.shp already extracted.\n",
      "tl_2024_04_place.zip already downloaded.\n",
      "tl_2024_04_place.shp already extracted.\n",
      "tl_2024_05_place.zip already downloaded.\n",
      "tl_2024_05_place.shp already extracted.\n",
      "tl_2024_06_place.zip already downloaded.\n",
      "tl_2024_06_place.shp already extracted.\n",
      "tl_2024_08_place.zip already downloaded.\n",
      "tl_2024_08_place.shp already extracted.\n",
      "tl_2024_09_place.zip already downloaded.\n",
      "tl_2024_09_place.shp already extracted.\n",
      "tl_2024_10_place.zip already downloaded.\n",
      "tl_2024_10_place.shp already extracted.\n",
      "tl_2024_11_place.zip already downloaded.\n",
      "tl_2024_11_place.shp already extracted.\n",
      "tl_2024_12_place.zip already downloaded.\n",
      "tl_2024_12_place.shp already extracted.\n",
      "tl_2024_13_place.zip already downloaded.\n",
      "tl_2024_13_place.shp already extracted.\n",
      "tl_2024_15_place.zip already downloaded.\n",
      "tl_2024_15_place.shp already extracted.\n",
      "tl_2024_16_place.zip already downloaded.\n",
      "tl_2024_16_place.shp already extracted.\n",
      "tl_2024_17_place.zip already downloaded.\n",
      "tl_2024_17_place.shp already extracted.\n",
      "tl_2024_18_place.zip already downloaded.\n",
      "tl_2024_18_place.shp already extracted.\n",
      "tl_2024_19_place.zip already downloaded.\n",
      "tl_2024_19_place.shp already extracted.\n",
      "tl_2024_20_place.zip already downloaded.\n",
      "tl_2024_20_place.shp already extracted.\n",
      "tl_2024_21_place.zip already downloaded.\n",
      "tl_2024_21_place.shp already extracted.\n",
      "tl_2024_22_place.zip already downloaded.\n",
      "tl_2024_22_place.shp already extracted.\n",
      "tl_2024_23_place.zip already downloaded.\n",
      "tl_2024_23_place.shp already extracted.\n",
      "tl_2024_24_place.zip already downloaded.\n",
      "tl_2024_24_place.shp already extracted.\n",
      "tl_2024_25_place.zip already downloaded.\n",
      "tl_2024_25_place.shp already extracted.\n",
      "tl_2024_26_place.zip already downloaded.\n",
      "tl_2024_26_place.shp already extracted.\n",
      "tl_2024_27_place.zip already downloaded.\n",
      "tl_2024_27_place.shp already extracted.\n",
      "tl_2024_28_place.zip already downloaded.\n",
      "tl_2024_28_place.shp already extracted.\n",
      "tl_2024_29_place.zip already downloaded.\n",
      "tl_2024_29_place.shp already extracted.\n",
      "tl_2024_30_place.zip already downloaded.\n",
      "tl_2024_30_place.shp already extracted.\n",
      "tl_2024_31_place.zip already downloaded.\n",
      "tl_2024_31_place.shp already extracted.\n",
      "tl_2024_32_place.zip already downloaded.\n",
      "tl_2024_32_place.shp already extracted.\n",
      "tl_2024_33_place.zip already downloaded.\n",
      "tl_2024_33_place.shp already extracted.\n",
      "tl_2024_34_place.zip already downloaded.\n",
      "tl_2024_34_place.shp already extracted.\n",
      "tl_2024_35_place.zip already downloaded.\n",
      "tl_2024_35_place.shp already extracted.\n",
      "tl_2024_36_place.zip already downloaded.\n",
      "tl_2024_36_place.shp already extracted.\n",
      "tl_2024_37_place.zip already downloaded.\n",
      "tl_2024_37_place.shp already extracted.\n",
      "tl_2024_38_place.zip already downloaded.\n",
      "tl_2024_38_place.shp already extracted.\n",
      "tl_2024_39_place.zip already downloaded.\n",
      "tl_2024_39_place.shp already extracted.\n",
      "tl_2024_40_place.zip already downloaded.\n",
      "tl_2024_40_place.shp already extracted.\n",
      "tl_2024_41_place.zip already downloaded.\n",
      "tl_2024_41_place.shp already extracted.\n",
      "tl_2024_42_place.zip already downloaded.\n",
      "tl_2024_42_place.shp already extracted.\n",
      "tl_2024_44_place.zip already downloaded.\n",
      "tl_2024_44_place.shp already extracted.\n",
      "tl_2024_45_place.zip already downloaded.\n",
      "tl_2024_45_place.shp already extracted.\n",
      "tl_2024_46_place.zip already downloaded.\n",
      "tl_2024_46_place.shp already extracted.\n",
      "tl_2024_47_place.zip already downloaded.\n",
      "tl_2024_47_place.shp already extracted.\n",
      "tl_2024_48_place.zip already downloaded.\n",
      "tl_2024_48_place.shp already extracted.\n",
      "tl_2024_49_place.zip already downloaded.\n",
      "tl_2024_49_place.shp already extracted.\n",
      "tl_2024_50_place.zip already downloaded.\n",
      "tl_2024_50_place.shp already extracted.\n",
      "tl_2024_51_place.zip already downloaded.\n",
      "tl_2024_51_place.shp already extracted.\n",
      "tl_2024_53_place.zip already downloaded.\n",
      "tl_2024_53_place.shp already extracted.\n",
      "tl_2024_54_place.zip already downloaded.\n",
      "tl_2024_54_place.shp already extracted.\n",
      "tl_2024_55_place.zip already downloaded.\n",
      "tl_2024_55_place.shp already extracted.\n",
      "tl_2024_56_place.zip already downloaded.\n",
      "tl_2024_56_place.shp already extracted.\n",
      "Downloading tl_2024_60_place.zip ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josiah\\miniconda3\\envs\\BigProject_1\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www2.census.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded tl_2024_60_place.zip\n",
      "Extracting tl_2024_60_place.zip ...\n",
      "Extracted shapefiles from tl_2024_60_place.zip\n",
      "Downloading tl_2024_66_place.zip ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josiah\\miniconda3\\envs\\BigProject_1\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www2.census.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded tl_2024_66_place.zip\n",
      "Extracting tl_2024_66_place.zip ...\n",
      "Extracted shapefiles from tl_2024_66_place.zip\n",
      "Downloading tl_2024_69_place.zip ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josiah\\miniconda3\\envs\\BigProject_1\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www2.census.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded tl_2024_69_place.zip\n",
      "Extracting tl_2024_69_place.zip ...\n",
      "Extracted shapefiles from tl_2024_69_place.zip\n",
      "Downloading tl_2024_72_place.zip ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josiah\\miniconda3\\envs\\BigProject_1\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www2.census.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded tl_2024_72_place.zip\n",
      "Extracting tl_2024_72_place.zip ...\n",
      "Extracted shapefiles from tl_2024_72_place.zip\n",
      "Downloading tl_2024_78_place.zip ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josiah\\miniconda3\\envs\\BigProject_1\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www2.census.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded tl_2024_78_place.zip\n",
      "Extracting tl_2024_78_place.zip ...\n",
      "Extracted shapefiles from tl_2024_78_place.zip\n",
      "Loading fire data...\n",
      "Loading states...\n",
      "Loading counties...\n",
      "Loading places...\n",
      "Mapping fire points to states...\n",
      "Mapping fire points to counties...\n",
      "Mapping fire points to places...\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T22:15:54.642602Z",
     "start_time": "2025-07-21T22:15:54.489921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = os.getcwd() + \"\\\\fire_data_enriched.csv\"\n",
    "\n",
    "data=pd.read_csv(path)\n",
    "data"
   ],
   "id": "a2f6d602d8c3a4cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Unnamed: 0        fire_id  fire_count  mean_confidence  state_name  \\\n",
       "0                0  2000-11-01_C0           4        72.250000    Virginia   \n",
       "1                1  2000-11-01_C1           5        56.200000    Kentucky   \n",
       "2                2  2000-11-01_C2           3        76.666667    Kentucky   \n",
       "3                3  2000-11-01_C3           3        55.333333     Georgia   \n",
       "4                4  2000-11-01_C4           3        89.000000    Kentucky   \n",
       "...            ...            ...         ...              ...         ...   \n",
       "126120      126120  2025-01-25_C0           4        88.750000      Hawaii   \n",
       "126121      126121  2025-01-25_C1           3        98.000000      Hawaii   \n",
       "126122      126122  2025-01-28_C0           3        92.666667      Hawaii   \n",
       "126123      126123  2025-01-28_C1           4        71.500000      Kansas   \n",
       "126124      126124  2025-01-30_C0           4        76.250000  California   \n",
       "\n",
       "       county_name place_name    acq_date cluster_confidence_binned  \\\n",
       "0          Madison        NaN  2000-11-01                   Nominal   \n",
       "1          Whitley        NaN  2000-11-01                   Nominal   \n",
       "2          Whitley        NaN  2000-11-01                   Nominal   \n",
       "3           Gordon        NaN  2000-11-01                   Nominal   \n",
       "4          Whitley        NaN  2000-11-01                      High   \n",
       "...            ...        ...         ...                       ...   \n",
       "126120      Hawaii        NaN  2025-01-25                      High   \n",
       "126121      Hawaii        NaN  2025-01-25                      High   \n",
       "126122      Hawaii        NaN  2025-01-28                      High   \n",
       "126123     Johnson    Gardner  2025-01-28                   Nominal   \n",
       "126124     Trinity        NaN  2025-01-30                   Nominal   \n",
       "\n",
       "       fire_count_binned  \n",
       "0                  Small  \n",
       "1                  Small  \n",
       "2                  Small  \n",
       "3                  Small  \n",
       "4                  Small  \n",
       "...                  ...  \n",
       "126120             Small  \n",
       "126121             Small  \n",
       "126122             Small  \n",
       "126123             Small  \n",
       "126124             Small  \n",
       "\n",
       "[126125 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fire_id</th>\n",
       "      <th>fire_count</th>\n",
       "      <th>mean_confidence</th>\n",
       "      <th>state_name</th>\n",
       "      <th>county_name</th>\n",
       "      <th>place_name</th>\n",
       "      <th>acq_date</th>\n",
       "      <th>cluster_confidence_binned</th>\n",
       "      <th>fire_count_binned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2000-11-01_C0</td>\n",
       "      <td>4</td>\n",
       "      <td>72.250000</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Madison</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>Nominal</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2000-11-01_C1</td>\n",
       "      <td>5</td>\n",
       "      <td>56.200000</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>Whitley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>Nominal</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2000-11-01_C2</td>\n",
       "      <td>3</td>\n",
       "      <td>76.666667</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>Whitley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>Nominal</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-11-01_C3</td>\n",
       "      <td>3</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Gordon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>Nominal</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2000-11-01_C4</td>\n",
       "      <td>3</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>Whitley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>High</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126120</th>\n",
       "      <td>126120</td>\n",
       "      <td>2025-01-25_C0</td>\n",
       "      <td>4</td>\n",
       "      <td>88.750000</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-25</td>\n",
       "      <td>High</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126121</th>\n",
       "      <td>126121</td>\n",
       "      <td>2025-01-25_C1</td>\n",
       "      <td>3</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-25</td>\n",
       "      <td>High</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126122</th>\n",
       "      <td>126122</td>\n",
       "      <td>2025-01-28_C0</td>\n",
       "      <td>3</td>\n",
       "      <td>92.666667</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-28</td>\n",
       "      <td>High</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126123</th>\n",
       "      <td>126123</td>\n",
       "      <td>2025-01-28_C1</td>\n",
       "      <td>4</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>Gardner</td>\n",
       "      <td>2025-01-28</td>\n",
       "      <td>Nominal</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126124</th>\n",
       "      <td>126124</td>\n",
       "      <td>2025-01-30_C0</td>\n",
       "      <td>4</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>California</td>\n",
       "      <td>Trinity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>Nominal</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126125 rows Ã— 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T04:52:08.907528Z",
     "start_time": "2025-07-20T04:52:07.005521Z"
    }
   },
   "cell_type": "code",
   "source": "data['state_name'].sort_values().unique()",
   "id": "5d6a59279e5fa4af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n",
       "       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
       "       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
       "       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
       "       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
       "       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n",
       "       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
       "       'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee',\n",
       "       'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "       'West Virginia', 'Wisconsin', 'Wyoming', nan], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T03:20:37.150794Z",
     "start_time": "2025-07-21T03:20:37.128425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_risk_factor(data, state, county=None, place=None, months=None, day=None):\n",
    "    month_map={'january':1,'february':2,'march':3,'april':4,'may':5,'june':6,'july':7,'august':8,'september':9,'october':10,'november':11,'december':12}\n",
    "    if months is not None:\n",
    "        for i in range(len(months)):\n",
    "            if months[i].lower() in month_map:\n",
    "                months[i] = month_map[months[i].lower()]\n",
    "\n",
    "    filtered = data[data['state_name'] == state].drop_duplicates(subset=['acq_date', 'fire_count_binned', 'confidence_binned'])\n",
    "    filtered['acq_date']=pd.to_datetime(filtered['acq_date'])\n",
    "    filtered['year'] = filtered['acq_date'].dt.year\n",
    "    filtered['month'] = filtered['acq_date'].dt.month\n",
    "    filtered = filtered[filtered['year'] != filtered['year'].max()]\n",
    "    filtered = filtered[filtered['year'] != filtered['year'].min()]\n",
    "    years=(filtered['year'].max()-filtered['year'].min())+1\n",
    "    total=years\n",
    "\n",
    "    if county is not None:\n",
    "        filtered = filtered[filtered['county_name'] == county]\n",
    "    if place is not None:\n",
    "        filtered = filtered[filtered['place_name'] == place]\n",
    "\n",
    "    if months is not None:\n",
    "        filtered = filtered[filtered['month'].isin(months)]\n",
    "        if len(months) == 1 and day is not None:\n",
    "            filtered = filtered[filtered['acq_date'].dt.day == day]\n",
    "        else:\n",
    "            filtered = filtered.drop_duplicates(subset=['month', 'year', 'fire_count_binned', 'confidence_binned'])\n",
    "            total=len(months)*years\n",
    "\n",
    "    else:\n",
    "        filtered = filtered.drop_duplicates(subset=['year', 'fire_count_binned', 'confidence_binned'])\n",
    "\n",
    "    filtered=filtered.groupby(['fire_count_binned', 'confidence_binned']).size().to_frame(name='count').reset_index()\n",
    "    filtered['frequency']=round((filtered['count']/total)*100,2)\n",
    "    filtered.drop('count',axis=1,inplace=True)\n",
    "\n",
    "    crosstab = pd.crosstab(\n",
    "    index=filtered['confidence_binned'],\n",
    "    columns=filtered['fire_count_binned'],\n",
    "    values=filtered['frequency'],\n",
    "    aggfunc='mean',\n",
    "    margins=False  # <-- no totals\n",
    "    )\n",
    "\n",
    "    # Remove axis names but keep labels visible\n",
    "    crosstab.index.name = None\n",
    "    crosstab.columns.name = None\n",
    "\n",
    "    # Style with dark grey borders and header background\n",
    "    styled_crosstab = (\n",
    "        crosstab.style\n",
    "        .format(\"{:.2f}%\")\n",
    "        .set_table_styles([\n",
    "            {'selector': 'td, th', 'props': 'border: 2px solid #333333; text-align: center;'},\n",
    "            {'selector': 'th', 'props': 'background-color: #555555; color: white;'}\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    return styled_crosstab\n"
   ],
   "id": "e775fb3111838adb",
   "outputs": [],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T03:23:54.562991Z",
     "start_time": "2025-07-21T03:23:54.505243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test=get_risk_factor(summary, state='Colorado', months=['August'])\n",
    "test"
   ],
   "id": "e9fbee5be7e7ad2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Moffat' 'Routt' 'Sedgwick' 'Park' 'Las Animas' 'Garfield' 'Teller'\n",
      " 'Jefferson' 'La Plata' 'Douglas' 'San Miguel' 'Montrose' 'Larimer'\n",
      " 'Rio Blanco' 'Jackson' 'Bent' 'Grand' 'Phillips' 'Delta' 'Montezuma'\n",
      " 'Archuleta' 'Mesa' 'Saguache' 'Baca' 'Ouray' 'Gunnison' 'El Paso' 'Eagle'\n",
      " 'Pueblo' 'Custer' 'Lake' 'Huerfano' 'Costilla' 'Chaffee' 'Conejos'\n",
      " 'Adams' 'Pitkin' 'Dolores' 'Weld' 'Boulder' 'Fremont' 'Yuma' 'Hinsdale'\n",
      " 'Mineral' 'Rio Grande' 'Prowers' 'Summit' 'Elbert' 'Clear Creek'\n",
      " 'San Juan' 'Alamosa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josiah\\AppData\\Local\\Temp\\ipykernel_14176\\2125224529.py:33: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  filtered=filtered.groupby(['fire_count_binned', 'confidence_binned']).size().to_frame(name='count').reset_index()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2ee348cc1a0>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_47084 td {\n",
       "  border: 2px solid #333333;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_47084  th {\n",
       "  border: 2px solid #333333;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_47084 th {\n",
       "  background-color: #555555;\n",
       "  color: white;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_47084\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_47084_level0_col0\" class=\"col_heading level0 col0\" >Small</th>\n",
       "      <th id=\"T_47084_level0_col1\" class=\"col_heading level0 col1\" >Medium</th>\n",
       "      <th id=\"T_47084_level0_col2\" class=\"col_heading level0 col2\" >Large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_47084_level0_row0\" class=\"row_heading level0 row0\" >Low</th>\n",
       "      <td id=\"T_47084_row0_col0\" class=\"data row0 col0\" >0.00%</td>\n",
       "      <td id=\"T_47084_row0_col1\" class=\"data row0 col1\" >0.00%</td>\n",
       "      <td id=\"T_47084_row0_col2\" class=\"data row0 col2\" >0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47084_level0_row1\" class=\"row_heading level0 row1\" >Nominal</th>\n",
       "      <td id=\"T_47084_row1_col0\" class=\"data row1 col0\" >68.18%</td>\n",
       "      <td id=\"T_47084_row1_col1\" class=\"data row1 col1\" >9.09%</td>\n",
       "      <td id=\"T_47084_row1_col2\" class=\"data row1 col2\" >0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47084_level0_row2\" class=\"row_heading level0 row2\" >High</th>\n",
       "      <td id=\"T_47084_row2_col0\" class=\"data row2 col0\" >50.00%</td>\n",
       "      <td id=\"T_47084_row2_col1\" class=\"data row2 col1\" >22.73%</td>\n",
       "      <td id=\"T_47084_row2_col2\" class=\"data row2 col2\" >4.55%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 192
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
